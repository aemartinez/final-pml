@book{cormen01,
 address = {{Cambridge, Mass}},
 edition = {2nd ed},
 editor = {Cormen, Thomas H. and Cormen, Thomas H.},
 file = {/Users/aemartinez/Zotero/storage/VY36J9EQ/Cormen and Cormen - 2001 - Introduction to algorithms.pdf},
 isbn = {978-0-262-03293-3},
 keywords = {Computer algorithms,Computer programming},
 langid = {english},
 lccn = {QA76.6 .I5858 2001},
 publisher = {{MIT Press}},
 title = {Introduction to Algorithms},
 year = {2001}
}

@book{hopcroft79,
 address = {{Reading, Mass}},
 author = {Hopcroft, John E. and Ullman, Jeffrey D.},
 isbn = {978-0-201-02988-8},
 keywords = {Computational complexity,Formal languages,Machine theory},
 lccn = {QA267 .H56},
 publisher = {{Addison-Wesley}},
 series = {Addison-{{Wesley}} Series in Computer Science},
 title = {Introduction to Automata Theory, Languages, and Computation},
 year = {1979}
}

@book{kearns94,
 address = {{Cambridge, Mass}},
 author = {Kearns, Michael J. and Vazirani, Umesh Virkumar},
 file = {/Users/aemartinez/Zotero/storage/TMQ6BCY8/Kearns and Vazirani - 1994 - An introduction to computational learning theory.pdf},
 isbn = {978-0-262-11193-5},
 keywords = {Algorithms,Artificial intelligence,Machine learning,Neural networks (Computer science)},
 lccn = {Q325.5 .K44 1994},
 publisher = {{MIT Press}},
 title = {An Introduction to Computational Learning Theory},
 year = {1994}
}

@book{mohri18,
 address = {{Cambridge, Massachusetts}},
 author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
 edition = {Second edition},
 file = {/Users/aemartinez/Zotero/storage/LTSSNLRV/Mohri et al. - 2018 - Foundations of machine learning.pdf},
 ids = {mohri18a},
 isbn = {978-0-262-03940-6},
 keywords = {Computer algorithms,Machine learning},
 lccn = {Q325.5 .M64 2018},
 publisher = {{The MIT Press}},
 series = {Adaptive Computation and Machine Learning},
 title = {Foundations of Machine Learning},
 year = {2018}
}

@article{schapire:ml-5_2,
 abstract = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class islearnable (orstrongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.},
 author = {Schapire, Robert E.},
 doi = {10.1007/BF00116037},
 file = {/Users/aemartinez/Zotero/storage/XP69VYQ2/Schapire - 1990 - The strength of weak learnability.pdf},
 issn = {1573-0565},
 journal = {Machine Learning},
 langid = {english},
 month = {June},
 number = {2},
 pages = {197--227},
 title = {The Strength of Weak Learnability},
 urldate = {2022-01-12},
 volume = {5},
 year = {1990}
}

@article{valiant:ca84,
 author = {Valiant, L. G.},
 doi = {10.1145/1968.1972},
 issn = {0001-0782},
 journal = {Communications of the ACM},
 keywords = {inductive inference,probabilistic models of learning,propositional expressions},
 month = {November},
 number = {11},
 pages = {1134--1142},
 title = {A Theory of the Learnable},
 urldate = {2021-03-22},
 volume = {27},
 year = {1984}
}

@article{vardi:ca10,
 author = {Vardi, Moshe Y.},
 doi = {10.1145/1839676.1839677},
 file = {/Users/aemartinez/Zotero/storage/BN9MENER/Vardi - 2010 - On P, NP, and computational complexity.pdf},
 issn = {0001-0782, 1557-7317},
 journal = {Communications of the ACM},
 langid = {english},
 month = {November},
 number = {11},
 pages = {5--5},
 title = {On {{P}}, {{NP}}, and Computational Complexity},
 urldate = {2021-03-10},
 volume = {53},
 year = {2010}
}
